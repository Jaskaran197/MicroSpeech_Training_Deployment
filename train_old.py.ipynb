{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple speech recognition to spot a limited number of keywords.\n",
    "This is a self-contained example script that will train a very basic audio\n",
    "recognition model in TensorFlow. It downloads the necessary training data and\n",
    "runs with reasonable defaults to train within a few hours even only using a CPU.\n",
    "For more information, please see\n",
    "https://www.tensorflow.org/tutorials/audio_recognition.\n",
    "It is intended as an introduction to using neural networks for audio\n",
    "recognition, and is not a full speech recognition system. For more advanced\n",
    "speech systems, I recommend looking into Kaldi. This network uses a keyword\n",
    "detection style to spot discrete words from a small vocabulary, consisting of\n",
    "\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", and \"go\".\n",
    "To run the training process, use:\n",
    "bazel run tensorflow/examples/speech_commands:train\n",
    "This will write out checkpoints to /tmp/speech_commands_train/, and will\n",
    "download over 1GB of open source training data, so you'll need enough free space\n",
    "and a good internet connection. The default data is a collection of thousands of\n",
    "one-second .wav files, each containing one spoken word. This data set is\n",
    "collected from https://aiyprojects.withgoogle.com/open_speech_recording, please\n",
    "consider contributing to help improve this and other models!\n",
    "As training progresses, it will print out its accuracy metrics, which should\n",
    "rise above 90% by the end. Once it's complete, you can run the freeze script to\n",
    "get a binary GraphDef that you can easily deploy on mobile applications.\n",
    "If you want to train on your own data, you'll need to create .wavs with your\n",
    "recordings, all at a consistent length, and then arrange them into subfolders\n",
    "organized by label. For example, here's a possible file structure:\n",
    "my_wavs >\n",
    "  up >\n",
    "    audio_0.wav\n",
    "    audio_1.wav\n",
    "  down >\n",
    "    audio_2.wav\n",
    "    audio_3.wav\n",
    "  other>\n",
    "    audio_4.wav\n",
    "    audio_5.wav\n",
    "You'll also need to tell the script what labels to look for, using the\n",
    "`--wanted_words` argument. In this case, 'up,down' might be what you want, and\n",
    "the audio in the 'other' folder would be used to train an 'unknown' category.\n",
    "To pull this all together, you'd run:\n",
    "bazel run tensorflow/examples/speech_commands:train -- \\\n",
    "--data_dir=my_wavs --wanted_words=up,down\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import input_data\n",
    "import modelss\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.contrib import slim as slim \n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  # We want to see all the logging messages for this tutorial.\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "  # Start a new TensorFlow session.\n",
    "  sess = tf.InteractiveSession()\n",
    "\n",
    "  # Begin by making sure we have the training data we need. If you already have\n",
    "  # training data of your own, use `--data_url= ` on the command line to avoid\n",
    "  # downloading.\n",
    "  model_settings = modelss.prepare_model_settings(\n",
    "      len(input_data.prepare_words_list(FLAGS.wanted_words.split(','))),\n",
    "      FLAGS.sample_rate, FLAGS.clip_duration_ms, FLAGS.window_size_ms,\n",
    "      FLAGS.window_stride_ms, FLAGS.dct_coefficient_count)\n",
    "  audio_processor = input_data.AudioProcessor(\n",
    "      FLAGS.data_url, FLAGS.data_dir, FLAGS.silence_percentage,\n",
    "      FLAGS.unknown_percentage,\n",
    "      FLAGS.wanted_words.split(','), FLAGS.validation_percentage,\n",
    "      FLAGS.testing_percentage, model_settings)\n",
    "  fingerprint_size = model_settings['fingerprint_size']\n",
    "  label_count = model_settings['label_count']\n",
    "  time_shift_samples = int((FLAGS.time_shift_ms * FLAGS.sample_rate) / 1000)\n",
    "  # Figure out the learning rates for each training phase. Since it's often\n",
    "  # effective to have high learning rates at the start of training, followed by\n",
    "  # lower levels towards the end, the number of steps and learning rates can be\n",
    "  # specified as comma-separated lists to define the rate at each stage. For\n",
    "  # example --how_many_training_steps=10000,3000 --learning_rate=0.001,0.0001\n",
    "  # will run 13,000 training loops in total, with a rate of 0.001 for the first\n",
    "  # 10,000, and 0.0001 for the final 3,000.\n",
    "  training_steps_list = list(map(int, FLAGS.how_many_training_steps.split(',')))\n",
    "  learning_rates_list = list(map(float, FLAGS.learning_rate.split(',')))\n",
    "  if len(training_steps_list) != len(learning_rates_list):\n",
    "    raise Exception(\n",
    "        '--how_many_training_steps and --learning_rate must be equal length '\n",
    "        'lists, but are %d and %d long instead' % (len(training_steps_list),\n",
    "                                                   len(learning_rates_list)))\n",
    "\n",
    "  fingerprint_input = tf.placeholder(\n",
    "      tf.float32, [None, fingerprint_size], name='fingerprint_input')\n",
    "\n",
    "  logits, dropout_prob = models.create_model(\n",
    "      fingerprint_input,\n",
    "      model_settings,\n",
    "      FLAGS.model_architecture,\n",
    "      FLAGS.model_size_info,\n",
    "      is_training=True)\n",
    "\n",
    "  # Define loss and optimizer\n",
    "  ground_truth_input = tf.placeholder(\n",
    "      tf.float32, [None, label_count], name='groundtruth_input')\n",
    "\n",
    "  # Optionally we can add runtime checks to spot when NaNs or other symptoms of\n",
    "  # numerical errors start occurring during training.\n",
    "  control_dependencies = []\n",
    "  if FLAGS.check_nans:\n",
    "    checks = tf.add_check_numerics_ops()\n",
    "    control_dependencies = [checks]\n",
    "\n",
    "  # Create the back propagation and training evaluation machinery in the graph.\n",
    "  with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy_mean = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=ground_truth_input, logits=logits))\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy_mean)\n",
    "\n",
    "  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "  with tf.name_scope('train'), tf.control_dependencies(update_ops), tf.control_dependencies(control_dependencies):\n",
    "    learning_rate_input = tf.placeholder(\n",
    "        tf.float32, [], name='learning_rate_input')\n",
    "    train_op = tf.train.AdamOptimizer(\n",
    "        learning_rate_input)\n",
    "    train_step = slim.learning.create_train_op(cross_entropy_mean, train_op)\n",
    "#    train_step = tf.train.GradientDescentOptimizer(\n",
    "#        learning_rate_input).minimize(cross_entropy_mean)\n",
    "  predicted_indices = tf.argmax(logits, 1)\n",
    "  expected_indices = tf.argmax(ground_truth_input, 1)\n",
    "  correct_prediction = tf.equal(predicted_indices, expected_indices)\n",
    "  confusion_matrix = tf.confusion_matrix(\n",
    "      expected_indices, predicted_indices, num_classes=label_count)\n",
    "  evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  tf.summary.scalar('accuracy', evaluation_step)\n",
    "\n",
    "  global_step = tf.train.get_or_create_global_step()\n",
    "  increment_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "  saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "  # Merge all the summaries and write them out to /tmp/retrain_logs (by default)\n",
    "  merged_summaries = tf.summary.merge_all()\n",
    "  train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                       sess.graph)\n",
    "  validation_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/validation')\n",
    "\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  # Parameter counts\n",
    "  params = tf.trainable_variables()\n",
    "  num_params = sum(map(lambda t: np.prod(tf.shape(t.value()).eval()), params))\n",
    "  print('Total number of Parameters: ', num_params)\n",
    "\n",
    "  start_step = 1\n",
    "\n",
    "  if FLAGS.start_checkpoint:\n",
    "    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\n",
    "    start_step = global_step.eval(session=sess)\n",
    "\n",
    "  tf.logging.info('Training from step: %d ', start_step)\n",
    "\n",
    "  # Save graph.pbtxt.\n",
    "  tf.train.write_graph(sess.graph_def, FLAGS.train_dir,\n",
    "                       FLAGS.model_architecture + '.pbtxt')\n",
    "\n",
    "  # Save list of words.\n",
    "  with gfile.GFile(\n",
    "      os.path.join(FLAGS.train_dir, FLAGS.model_architecture + '_labels.txt'),\n",
    "      'w') as f:\n",
    "    f.write('\\n'.join(audio_processor.words_list))\n",
    "\n",
    "  # Training loop.\n",
    "  best_accuracy = 0\n",
    "  training_steps_max = np.sum(training_steps_list)\n",
    "  for training_step in xrange(start_step, training_steps_max + 1):\n",
    "    # Figure out what the current learning rate is.\n",
    "    training_steps_sum = 0\n",
    "    for i in range(len(training_steps_list)):\n",
    "      training_steps_sum += training_steps_list[i]\n",
    "      if training_step <= training_steps_sum:\n",
    "        learning_rate_value = learning_rates_list[i]\n",
    "        break\n",
    "    # Pull the audio samples we'll use for training.\n",
    "    train_fingerprints, train_ground_truth = audio_processor.get_data(\n",
    "        FLAGS.batch_size, 0, model_settings, FLAGS.background_frequency,\n",
    "        FLAGS.background_volume, time_shift_samples, 'training', sess)\n",
    "    # Run the graph with this batch of training data.\n",
    "    train_summary, train_accuracy, cross_entropy_value, _, _ = sess.run(\n",
    "        [\n",
    "            merged_summaries, evaluation_step, cross_entropy_mean, train_step,\n",
    "            increment_global_step\n",
    "        ],\n",
    "        feed_dict={\n",
    "            fingerprint_input: train_fingerprints,\n",
    "            ground_truth_input: train_ground_truth,\n",
    "            learning_rate_input: learning_rate_value,\n",
    "            dropout_prob: 1.0\n",
    "        })\n",
    "    train_writer.add_summary(train_summary, training_step)\n",
    "    tf.logging.info('Step #%d: rate %f, accuracy %.2f%%, cross entropy %f' %\n",
    "                    (training_step, learning_rate_value, train_accuracy * 100,\n",
    "                     cross_entropy_value))\n",
    "    is_last_step = (training_step == training_steps_max)\n",
    "    if (training_step % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
    "      set_size = audio_processor.set_size('validation')\n",
    "      total_accuracy = 0\n",
    "      total_conf_matrix = None\n",
    "      for i in xrange(0, set_size, FLAGS.batch_size):\n",
    "        validation_fingerprints, validation_ground_truth = (\n",
    "            audio_processor.get_data(FLAGS.batch_size, i, model_settings, 0.0,\n",
    "                                     0.0, 0, 'validation', sess))\n",
    "\n",
    "        # Run a validation step and capture training summaries for TensorBoard\n",
    "        # with the `merged` op.\n",
    "        validation_summary, validation_accuracy, conf_matrix = sess.run(\n",
    "            [merged_summaries, evaluation_step, confusion_matrix],\n",
    "            feed_dict={\n",
    "                fingerprint_input: validation_fingerprints,\n",
    "                ground_truth_input: validation_ground_truth,\n",
    "                dropout_prob: 1.0\n",
    "            })\n",
    "        validation_writer.add_summary(validation_summary, training_step)\n",
    "        batch_size = min(FLAGS.batch_size, set_size - i)\n",
    "        total_accuracy += (validation_accuracy * batch_size) / set_size\n",
    "        if total_conf_matrix is None:\n",
    "          total_conf_matrix = conf_matrix\n",
    "        else:\n",
    "          total_conf_matrix += conf_matrix\n",
    "      tf.logging.info('Confusion Matrix:\\n %s' % (total_conf_matrix))\n",
    "      tf.logging.info('Step %d: Validation accuracy = %.2f%% (N=%d)' %\n",
    "                      (training_step, total_accuracy * 100, set_size))\n",
    "\n",
    "      # Save the model checkpoint when validation accuracy improves\n",
    "      if total_accuracy > best_accuracy:\n",
    "        best_accuracy = total_accuracy\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, 'best',\n",
    "                                       FLAGS.model_architecture + '_'+ str(int(best_accuracy*10000)) + '.ckpt')\n",
    "        tf.logging.info('Saving best model to \"%s-%d\"', checkpoint_path, training_step)\n",
    "        saver.save(sess, checkpoint_path, global_step=training_step)\n",
    "      tf.logging.info('So far the best validation accuracy is %.2f%%' % (best_accuracy*100))\n",
    "\n",
    "  set_size = audio_processor.set_size('testing')\n",
    "  tf.logging.info('set_size=%d', set_size)\n",
    "  total_accuracy = 0\n",
    "  total_conf_matrix = None\n",
    "  for i in xrange(0, set_size, FLAGS.batch_size):\n",
    "    test_fingerprints, test_ground_truth = audio_processor.get_data(\n",
    "        FLAGS.batch_size, i, model_settings, 0.0, 0.0, 0, 'testing', sess)\n",
    "    test_accuracy, conf_matrix = sess.run(\n",
    "        [evaluation_step, confusion_matrix],\n",
    "        feed_dict={\n",
    "            fingerprint_input: test_fingerprints,\n",
    "            ground_truth_input: test_ground_truth,\n",
    "            dropout_prob: 1.0\n",
    "        })\n",
    "    batch_size = min(FLAGS.batch_size, set_size - i)\n",
    "    total_accuracy += (test_accuracy * batch_size) / set_size\n",
    "    if total_conf_matrix is None:\n",
    "      total_conf_matrix = conf_matrix\n",
    "    else:\n",
    "      total_conf_matrix += conf_matrix\n",
    "  tf.logging.info('Confusion Matrix:\\n %s' % (total_conf_matrix))\n",
    "  tf.logging.info('Final test accuracy = %.2f%% (N=%d)' % (total_accuracy * 100,\n",
    "                                                           set_size))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--data_url',\n",
    "      type=str,\n",
    "      # pylint: disable=line-too-long\n",
    "      default='http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n",
    "      # pylint: enable=line-too-long\n",
    "      help='Location of speech training data archive on the web.')\n",
    "  parser.add_argument(\n",
    "      '--data_dir',\n",
    "      type=str,\n",
    "      default='/tmp/speech_dataset/',\n",
    "      help=\"\"\"\\\n",
    "      Where to download the speech training data to.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--background_volume',\n",
    "      type=float,\n",
    "      default=0.1,\n",
    "      help=\"\"\"\\\n",
    "      How loud the background noise should be, between 0 and 1.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--background_frequency',\n",
    "      type=float,\n",
    "      default=0.8,\n",
    "      help=\"\"\"\\\n",
    "      How many of the training samples have background noise mixed in.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--silence_percentage',\n",
    "      type=float,\n",
    "      default=10.0,\n",
    "      help=\"\"\"\\\n",
    "      How much of the training data should be silence.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--unknown_percentage',\n",
    "      type=float,\n",
    "      default=10.0,\n",
    "      help=\"\"\"\\\n",
    "      How much of the training data should be unknown words.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--time_shift_ms',\n",
    "      type=float,\n",
    "      default=100.0,\n",
    "      help=\"\"\"\\\n",
    "      Range to randomly shift the training audio by in time.\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      '--testing_percentage',\n",
    "      type=int,\n",
    "      default=10,\n",
    "      help='What percentage of wavs to use as a test set.')\n",
    "  parser.add_argument(\n",
    "      '--validation_percentage',\n",
    "      type=int,\n",
    "      default=10,\n",
    "      help='What percentage of wavs to use as a validation set.')\n",
    "  parser.add_argument(\n",
    "      '--sample_rate',\n",
    "      type=int,\n",
    "      default=16000,\n",
    "      help='Expected sample rate of the wavs',)\n",
    "  parser.add_argument(\n",
    "      '--clip_duration_ms',\n",
    "      type=int,\n",
    "      default=1000,\n",
    "      help='Expected duration in milliseconds of the wavs',)\n",
    "  parser.add_argument(\n",
    "      '--window_size_ms',\n",
    "      type=float,\n",
    "      default=30.0,\n",
    "      help='How long each spectrogram timeslice is',)\n",
    "  parser.add_argument(\n",
    "      '--window_stride_ms',\n",
    "      type=float,\n",
    "      default=20.0,\n",
    "      help='How long each spectrogram timeslice is',)\n",
    "  parser.add_argument(\n",
    "      '--dct_coefficient_count',\n",
    "      type=int,\n",
    "      default=40,\n",
    "      help='How many bins to use for the MFCC fingerprint',)\n",
    "  parser.add_argument(\n",
    "      '--how_many_training_steps',\n",
    "      type=str,\n",
    "      default='15000,3000',\n",
    "      help='How many training loops to run',)\n",
    "  parser.add_argument(\n",
    "      '--eval_step_interval',\n",
    "      type=int,\n",
    "      default=400,\n",
    "      help='How often to evaluate the training results.')\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type=str,\n",
    "      default='0.001,0.0001',\n",
    "      help='How large a learning rate to use when training.')\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='How many items to train with at once',)\n",
    "  parser.add_argument(\n",
    "      '--summaries_dir',\n",
    "      type=str,\n",
    "      default='/tmp/retrain_logs',\n",
    "      help='Where to save summary logs for TensorBoard.')\n",
    "  parser.add_argument(\n",
    "      '--wanted_words',\n",
    "      type=str,\n",
    "      default='up,down,left,right,on,off',\n",
    "      help='Words to use (others will be added to an unknown label)',)\n",
    "  parser.add_argument(\n",
    "      '--train_dir',\n",
    "      type=str,\n",
    "      default='/tmp/speech_commands_train',\n",
    "      help='Directory to write event logs and checkpoint.')\n",
    "  parser.add_argument(\n",
    "      '--save_step_interval',\n",
    "      type=int,\n",
    "      default=100,\n",
    "      help='Save model checkpoint every save_steps.')\n",
    "  parser.add_argument(\n",
    "      '--start_checkpoint',\n",
    "      type=str,\n",
    "      default='',\n",
    "      help='If specified, restore this pretrained model before any training.')\n",
    "  parser.add_argument(\n",
    "      '--model_architecture',\n",
    "      type=str,\n",
    "      default='dnn',\n",
    "      help='What model architecture to use')\n",
    "  parser.add_argument(\n",
    "      '--model_size_info',\n",
    "      type=int,\n",
    "      nargs=\"+\",\n",
    "      default=[128,128,128],\n",
    "      help='Model dimensions - different for various models')\n",
    "  parser.add_argument(\n",
    "      '--check_nans',\n",
    "      type=bool,\n",
    "      default=False,\n",
    "      help='Whether to check for invalid numbers during processing')\n",
    "\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (microai3.6)",
   "language": "python",
   "name": "microai3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
